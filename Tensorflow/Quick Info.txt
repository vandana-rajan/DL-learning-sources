# Some quick info about TF I learnt from coursera course on TF [https://www.coursera.org/learn/intro-tensorflow]

Tensors are one step above matrices. So you have a scalar, then a vector, then a matrix and then next comes tensors like 3D tensor, 4D tensor etc.

Tensorflow functions using Directed Acyclic Graphs (DAGs). 

The nodes of this graph are operations like add, multiply or softmax etc and the edges are tensors. 
Computations are performed by the flow of tensors over this graph and hence the name TensorFlow.

TF performs lazy computations. For example

>> a = tf.constant([1,2,3])
>> b = tf.constant([4,5,6])
>> c = tf.add(a,b)

This does not produce the value of c immediately. Rather, it will create a tensor for c.

>>> c
<tf.Tensor 'Add:0' shape=(3,) dtype=int32>

In order to find the actual value of c, we need to create a TF session and run that session.

>> with tf.Session() as sess:
>>      sess.run(c)

This will give the output of c

>> array([4, 6, 8], dtype=int32)

Instead of sess.run(c), c.eval() would also give the same result. 

Now for development purposes where we need to examine the values of tensors immediately, we can use 'eager evaluation'. Note that
this should be used only for development purposes and not for production code.

Example:

>>> import tensorflow as tf

>>> from tensorflow.contrib.eager.python import tfe

>>> tfe.enable_eager_execution()

>>> a = tf.constant([1,2,3])

>>> b = tf.constant([3,4,5])

>>> c = tf.add(a,b)

>>> c
<tf.Tensor: id=2, shape=(3,), dtype=int32, numpy=array([4, 6, 8], dtype=int32)>

Now that we know how to create a TF graph, we need to visualize the graph as well. We can use 'tf.summary.FileWriter' for this.

Example,

>> with tf.Session() as sess:

>>      with tf.summary.FileWriter('summaries',sess.graph) as writer:

>>            c1 = sess.run(c)

This will create a directory called summaries which contain an event file. Now we need 'tensorboard' to visualize this graph.

Tensors can be stacked on top of each other using 'tf.stack' command. They can also be sliced by specifying the rows and columns.

Examples:

>> x[:,1]

>> x[1, 0:2]

Tensors can be reshaped using 'tf.reshape' function. 

>> x = tf.constant([[3,5,7],[4,6,8]])

>> y = tf.reshape(x,[3,2])

>>> with tf.Session() as sess:
...     sess.run(y)
... 
array([[3, 5],
       [7, 4],
       [6, 8]], dtype=int32)

Note here that TF goes through x by taking elements from each row.

So far we have seen how to create and use constants in TF. Now lets look at variables, whose values change as the programme evolves.

We use 'tf.get_variable' to create a variable in TF. When creating the variable, we can specify the scope of that variable using 
'tf.variable_scope'. While creating a variable, we have to specify how to initialise that variable using 'tf.truncated_normal_initializer()'. Inside the session, remember to initialize all the variables using 'tf.global_variables_initializer()'.

We can use placeholders to specify values during session. Use 'tf.placeholder' for that.

Example:

>>> a = tf.placeholder("float",None)
>>> b = a*4
>>> print a
Tensor("Placeholder:0", dtype=float32)
>>> with tf.Session() as sess:
...     print(sess.run(b, feed_dict={a:[1,2,3]}))
... 
[ 4.  8. 12.]

How to debug a TF program? For one, we have the eager execution mode. But we need to know how to debug in lazy evaluation mode.
To get the shape of a tensor, we can use 'get_shape' method.

>> a = data[:,0:2]
>> a.get_shape()

Manipulate the shape of tensors using various commands like 'tf.reshape','tf.expand_dims','tf.slice','tf.squeeze'.
Use 'tf.cast' to cast from one data type to another.

To debug full blown programmes, we have 3 methods

1. tf.Print

2. tfdbg

3. tensorboard

Also, change logging level to INFO to get more information about errors. 

>> tf.logging.set_verbosity(tf.logging.INFO)

There are different levels of verbosity - debug, info, warn, error and fatal. We get more verbose output with debug and least with fatal.

We can use tensorflow debugger to create breakpoints and step through the code.

Estimator API - tf.estimator

Tensorflow has a number of pre-made estimators. TF.estimator can also be used with our custom models as well.
We can use 'tf.feature_column' API to define features that are to input to the models.

Model checkpoints - They allow us to continue training, resume on failure and predict from trained model.
Restarting from check point is the default behavior of estimators.

We can load smaller datasets into memory very easily. But what about bigger datasets?
Larger datasets can be split and put inside multiple files that can be loaded progressively. Since we are training in batches, one mini-batch is all we need at a time in memory.

'tf.data.Dataset' can be used for input to 'tf.estimator'.














